{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Установка пакетов"
      ],
      "metadata": {
        "id": "Jobb_VK27jmw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj7g7abeH9R0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enX815X_t1dj",
        "outputId": "5454dcbd-a9e9-4e43-c859-7881f78aac1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ru-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.8.0/ru_core_news_sm-3.8.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pymorphy3>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ru-core-news-sm==3.8.0) (2.0.2)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (0.7.2)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.10/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (2.4.417150.4580142)\n",
            "Installing collected packages: ru-core-news-sm\n",
            "  Attempting uninstall: ru-core-news-sm\n",
            "    Found existing installation: ru-core-news-sm 3.7.0\n",
            "    Uninstalling ru-core-news-sm-3.7.0:\n",
            "      Successfully uninstalled ru-core-news-sm-3.7.0\n",
            "Successfully installed ru-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download ru_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Инициализация глобальных переменных"
      ],
      "metadata": {
        "id": "Yq_jGy1E7oAW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSz88WckIA7D",
        "outputId": "7ddb74cf-ecf5-4f47-a3f3-535c080d510c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('russian'))\n",
        "nlp = spacy.load(\"ru_core_news_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Системный метод"
      ],
      "metadata": {
        "id": "Y4OBjZDf8uqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_processed_chunks_folder(folder_path='second/processed_chunks'):\n",
        "    if os.path.exists(folder_path):\n",
        "        for filename in os.listdir(folder_path):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            try:\n",
        "                if os.path.isfile(file_path):\n",
        "                    os.remove(file_path)\n",
        "            except Exception as e:\n",
        "                print(f\"Не удалось удалить файл {file_path}: {e}\")"
      ],
      "metadata": {
        "id": "a2DiEN9y71U-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Методы для работы с текстом"
      ],
      "metadata": {
        "id": "LGYn2hw47uQ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZM3frycjfNh"
      },
      "outputs": [],
      "source": [
        "def get_word_counts(text_series):\n",
        "    all_text = ' '.join(text_series)\n",
        "    words = all_text.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    word_counts = Counter(words)\n",
        "    return word_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOYSiC8pILRQ"
      },
      "outputs": [],
      "source": [
        "def search_top_words(word_counts, top_n=10):\n",
        "    most_common = word_counts.most_common(top_n)\n",
        "    top_words = set(word for word, _ in most_common)\n",
        "    return top_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_sM4DrOjpTC"
      },
      "outputs": [],
      "source": [
        "def lemmatize_words(texts):\n",
        "    docs = nlp.pipe(texts, disable=[\"parser\", \"ner\"])\n",
        "    return [' '.join([token.lemma_ for token in doc]) for doc in docs]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Методы процессов обработки текста и тд\n"
      ],
      "metadata": {
        "id": "8_pKrA1i8PbU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDszQkf5vC5M"
      },
      "outputs": [],
      "source": [
        "def process_chunk(chunk, chunk_index):\n",
        "    chunk = chunk.assign(\n",
        "        cleaned_text=chunk['text'].str.replace(r'[^\\w\\s]', '', regex=True).str.lower().str.strip()\n",
        "    )\n",
        "\n",
        "    word_counts = get_word_counts(chunk['cleaned_text'])\n",
        "    top_words = search_top_words(word_counts, top_n=10)\n",
        "\n",
        "    chunk['cleaned_text'] = chunk['cleaned_text'].str.split().apply(lambda words: ' '.join([word for word in words if word not in top_words]))\n",
        "\n",
        "    chunk['tokens'] = chunk['cleaned_text'].str.split()\n",
        "\n",
        "    chunk['lemmatized'] = lemmatize_words(chunk['cleaned_text'].tolist())\n",
        "\n",
        "    chunk.to_csv(f'second/processed_chunks/processed_chunk_{chunk_index}.csv', index=False)\n",
        "\n",
        "    return chunk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text(file_path, num_chunks=20):\n",
        "    clear_processed_chunks_folder()\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    chunk_size = len(df) // num_chunks + (len(df) % num_chunks > 0)\n",
        "    chunks = [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
        "\n",
        "    print(f\"Processing {len(chunks)} chunks\")\n",
        "\n",
        "    processed_chunks = []\n",
        "    for index, chunk in enumerate(chunks):\n",
        "        processed_chunk = process_chunk(chunk, index)\n",
        "        processed_chunks.append(processed_chunk)\n",
        "\n",
        "    final_df = pd.concat(processed_chunks, ignore_index=True)\n",
        "    clear_processed_chunks_folder()\n",
        "    return final_df"
      ],
      "metadata": {
        "id": "yReXVmwG8bIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Головной вызов методов"
      ],
      "metadata": {
        "id": "X7_SrjzD8f8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    try:\n",
        "        processed_df = process_text('second/combined_texts.csv')\n",
        "        processed_df.to_csv('second/processed_texts.csv', index=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Произошла ошибка: {e}\")"
      ],
      "metadata": {
        "id": "NYwJPgit8fYW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}